{
  "models": [
    {
      "id": "meta-llama/llama-1",
      "name": "LLaMA",
      "shortName": "LLaMA 1",
      "company": "Meta",
      "releaseDate": "2023-02-24",
      "parameters": "7B/13B/33B/65B",
      "parametersRaw": 65000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "research-only",
      "licenseType": "open-weight",
      "contextWindow": 2048,
      "tags": ["llama", "meta", "landmark"],
      "paperUrl": "https://arxiv.org/abs/2302.13971",
      "description": "Meta's first open-weight LLM. Showed that smaller models trained on more data can match larger ones.",
      "significance": "Ignited the open-source LLM revolution",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "stanford/alpaca",
      "name": "Alpaca",
      "shortName": "Alpaca",
      "company": "Stanford",
      "releaseDate": "2023-03-13",
      "parameters": "7B",
      "parametersRaw": 7000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "cc-by-nc-4.0",
      "licenseType": "open-weight",
      "contextWindow": 2048,
      "tags": ["alpaca", "stanford", "instruction-tuned"],
      "description": "Stanford's instruction-following model fine-tuned from LLaMA 7B using 52K instruction-following demonstrations.",
      "significance": "Showed that fine-tuning on cheap synthetic data creates powerful instruction followers",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "lmsys/vicuna",
      "name": "Vicuna",
      "shortName": "Vicuna",
      "company": "LMSYS",
      "releaseDate": "2023-03-30",
      "parameters": "13B",
      "parametersRaw": 13000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "cc-by-nc-4.0",
      "licenseType": "open-weight",
      "contextWindow": 2048,
      "tags": ["vicuna", "lmsys", "sharegpt"],
      "description": "Fine-tuned LLaMA on user-shared ChatGPT conversations from ShareGPT.",
      "significance": "Achieved ~90% of ChatGPT quality with open weights",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "berkeley/koala",
      "name": "Koala",
      "shortName": "Koala",
      "company": "UC Berkeley",
      "releaseDate": "2023-04-03",
      "parameters": "13B",
      "parametersRaw": 13000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "research-only",
      "licenseType": "open-weight",
      "contextWindow": 2048,
      "tags": ["koala", "berkeley"],
      "description": "Fine-tuned LLaMA on dialogue data from the web.",
      "significance": "Early academic fine-tune of LLaMA",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "microsoft/wizardlm",
      "name": "WizardLM",
      "shortName": "WizardLM",
      "company": "Microsoft",
      "releaseDate": "2023-05-01",
      "parameters": "7B",
      "parametersRaw": 7000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "research-only",
      "licenseType": "open-weight",
      "contextWindow": 2048,
      "tags": ["wizardlm", "microsoft", "evol-instruct"],
      "paperUrl": "https://arxiv.org/abs/2304.12244",
      "description": "Used Evol-Instruct to automatically create complex instruction data for fine-tuning LLaMA.",
      "significance": "Pioneered Evol-Instruct method for synthetic data generation",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-2",
      "name": "Llama 2",
      "shortName": "Llama 2",
      "company": "Meta",
      "releaseDate": "2023-07-18",
      "parameters": "7B/13B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama2",
      "licenseType": "open-weight",
      "contextWindow": 4096,
      "tags": ["llama", "meta"],
      "huggingfaceUrl": "https://huggingface.co/meta-llama/Llama-2-70b",
      "paperUrl": "https://arxiv.org/abs/2307.09288",
      "blogUrl": "https://ai.meta.com/llama/",
      "description": "Second generation Llama. Trained on 2T tokens with commercial use license.",
      "significance": "First commercially usable open-weight LLM at scale",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-2-chat",
      "name": "Llama 2 Chat",
      "shortName": "Llama 2 Chat",
      "company": "Meta",
      "releaseDate": "2023-07-18",
      "parameters": "7B/13B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama2",
      "licenseType": "open-weight",
      "contextWindow": 4096,
      "tags": ["llama", "meta", "chat", "rlhf"],
      "description": "RLHF-tuned chat variant of Llama 2.",
      "significance": "Open-weight chat model competitive with ChatGPT",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/code-llama",
      "name": "Code Llama",
      "shortName": "Code Llama",
      "company": "Meta",
      "releaseDate": "2023-08-24",
      "parameters": "7B/13B/34B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "code",
      "license": "llama2",
      "licenseType": "open-weight",
      "contextWindow": 16384,
      "tags": ["llama", "meta", "code"],
      "paperUrl": "https://arxiv.org/abs/2308.12950",
      "description": "Code-specialized variant of Llama 2 with infilling and long-context support.",
      "significance": "First major open code LLM from a top lab",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/code-llama-instruct",
      "name": "Code Llama Instruct",
      "shortName": "Code Llama Instruct",
      "company": "Meta",
      "releaseDate": "2023-08-24",
      "parameters": "7B/13B/34B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "code",
      "license": "llama2",
      "licenseType": "open-weight",
      "contextWindow": 16384,
      "tags": ["llama", "meta", "code", "instruct"],
      "description": "Instruction-tuned Code Llama for code generation and explanation tasks.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3",
      "name": "Llama 3",
      "shortName": "Llama 3",
      "company": "Meta",
      "releaseDate": "2024-04-18",
      "parameters": "8B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3",
      "licenseType": "open-weight",
      "contextWindow": 8192,
      "tags": ["llama", "meta"],
      "huggingfaceUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
      "blogUrl": "https://ai.meta.com/blog/meta-llama-3/",
      "description": "Third generation Llama with new tokenizer and trained on 15T+ tokens.",
      "significance": "Massive training data scale leap for open models",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3-instruct",
      "name": "Llama 3 Instruct",
      "shortName": "Llama 3 Instruct",
      "company": "Meta",
      "releaseDate": "2024-04-18",
      "parameters": "8B/70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3",
      "licenseType": "open-weight",
      "contextWindow": 8192,
      "tags": ["llama", "meta", "instruct"],
      "description": "Instruction-tuned variant of Llama 3.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3.1",
      "name": "Llama 3.1",
      "shortName": "Llama 3.1",
      "company": "Meta",
      "releaseDate": "2024-07-23",
      "parameters": "8B/70B/405B",
      "parametersRaw": 405000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3.1",
      "licenseType": "open-weight",
      "contextWindow": 131072,
      "tags": ["llama", "meta", "multilingual"],
      "huggingfaceUrl": "https://huggingface.co/meta-llama/Llama-3.1-405B",
      "paperUrl": "https://arxiv.org/abs/2407.21783",
      "blogUrl": "https://ai.meta.com/blog/meta-llama-3-1/",
      "description": "Llama 3.1 with 128K context, multilingual support, and 405B flagship.",
      "significance": "Largest open-weight model ever released (405B)",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3.1-instruct",
      "name": "Llama 3.1 Instruct",
      "shortName": "Llama 3.1 Instruct",
      "company": "Meta",
      "releaseDate": "2024-07-23",
      "parameters": "8B/70B/405B",
      "parametersRaw": 405000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3.1",
      "licenseType": "open-weight",
      "contextWindow": 131072,
      "tags": ["llama", "meta", "instruct"],
      "description": "Instruction-tuned Llama 3.1 with tool use capabilities.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3.2",
      "name": "Llama 3.2",
      "shortName": "Llama 3.2",
      "company": "Meta",
      "releaseDate": "2024-09-25",
      "parameters": "1B/3B/11B/90B",
      "parametersRaw": 90000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3.2",
      "licenseType": "open-weight",
      "contextWindow": 131072,
      "tags": ["llama", "meta", "lightweight", "on-device"],
      "description": "Lightweight and multimodal Llama models, including 1B and 3B for on-device use.",
      "significance": "First on-device Llama models",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3.2-vision",
      "name": "Llama 3.2 Vision",
      "shortName": "Llama 3.2 Vision",
      "company": "Meta",
      "releaseDate": "2024-09-25",
      "parameters": "11B/90B",
      "parametersRaw": 90000000000,
      "architecture": "MllamaForConditionalGeneration",
      "architectureFamily": "Transformer (Decoder-only, Multimodal)",
      "modality": "text+vision",
      "license": "llama3.2",
      "licenseType": "open-weight",
      "contextWindow": 131072,
      "tags": ["llama", "meta", "vision", "multimodal"],
      "description": "Multimodal variant of Llama 3.2 with vision understanding capabilities.",
      "significance": "First open multimodal Llama",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-3.3",
      "name": "Llama 3.3",
      "shortName": "Llama 3.3",
      "company": "Meta",
      "releaseDate": "2024-12-06",
      "parameters": "70B",
      "parametersRaw": 70000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "llama3.3",
      "licenseType": "open-weight",
      "contextWindow": 131072,
      "tags": ["llama", "meta"],
      "description": "Optimized 70B Llama matching the 405B's performance at a fraction of the cost.",
      "significance": "70B model matching 405B quality through better training",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-4",
      "name": "Llama 4",
      "shortName": "Llama 4",
      "company": "Meta",
      "releaseDate": "2025-04-05",
      "parameters": "MoE",
      "parametersRaw": null,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (MoE)",
      "modality": "text+vision",
      "license": "llama4",
      "licenseType": "open-weight",
      "contextWindow": 1000000,
      "tags": ["llama", "meta", "moe"],
      "description": "Fourth generation Llama using Mixture-of-Experts architecture.",
      "significance": "Meta's first MoE architecture Llama",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-4-scout",
      "name": "Llama 4 Scout",
      "shortName": "Llama 4 Scout",
      "company": "Meta",
      "releaseDate": "2025-04-05",
      "parameters": "17Bx16E (109B total)",
      "parametersRaw": 109000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (MoE)",
      "modality": "text+vision",
      "license": "llama4",
      "licenseType": "open-weight",
      "contextWindow": 1000000,
      "tags": ["llama", "meta", "moe", "scout"],
      "description": "Llama 4 Scout with 17B active params and 16 experts, 10M context support.",
      "significance": "First natively multimodal open MoE model",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta-llama/llama-4-maverick",
      "name": "Llama 4 Maverick",
      "shortName": "Llama 4 Maverick",
      "company": "Meta",
      "releaseDate": "2025-04-05",
      "parameters": "17Bx128E (400B+ total)",
      "parametersRaw": 400000000000,
      "architecture": "LlamaForCausalLM",
      "architectureFamily": "Transformer (MoE)",
      "modality": "text+vision",
      "license": "llama4",
      "licenseType": "open-weight",
      "contextWindow": 1000000,
      "tags": ["llama", "meta", "moe", "maverick"],
      "description": "Llama 4 Maverick with 128 experts, the largest Llama 4 model.",
      "significance": "Massive MoE model rivaling GPT-4 class",
      "source": "curated",
      "lastVerified": "2025-06-15"
    }
  ],
  "relationships": [
    { "parent": "meta-llama/llama-1", "child": "stanford/alpaca", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-1", "child": "lmsys/vicuna", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-1", "child": "berkeley/koala", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-1", "child": "microsoft/wizardlm", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-1", "child": "meta-llama/llama-2", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-2", "child": "meta-llama/llama-2-chat", "type": "finetune", "method": "RLHF", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-2", "child": "meta-llama/code-llama", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/code-llama", "child": "meta-llama/code-llama-instruct", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-2", "child": "meta-llama/llama-3", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3", "child": "meta-llama/llama-3-instruct", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3", "child": "meta-llama/llama-3.1", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3.1", "child": "meta-llama/llama-3.1-instruct", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3.1", "child": "meta-llama/llama-3.2", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3.2", "child": "meta-llama/llama-3.2-vision", "type": "multimodal-extension", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3.2", "child": "meta-llama/llama-3.3", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-3.1", "child": "meta-llama/llama-4", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-4", "child": "meta-llama/llama-4-scout", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "meta-llama/llama-4", "child": "meta-llama/llama-4-maverick", "type": "base", "source": "curated", "confidence": "high" }
  ]
}
