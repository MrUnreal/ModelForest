{
  "models": [
    {
      "id": "google/transformer",
      "name": "Transformer",
      "shortName": "Transformer",
      "company": "Google",
      "releaseDate": "2017-06-12",
      "parameters": "N/A",
      "parametersRaw": null,
      "architecture": "Transformer",
      "architectureFamily": "Transformer (Encoder-Decoder)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "tags": ["transformer", "google", "seminal"],
      "paperUrl": "https://arxiv.org/abs/1706.03762",
      "description": "The original Transformer architecture from 'Attention Is All You Need'.",
      "significance": "Foundation of all modern LLMs — introduced self-attention",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/bert",
      "name": "BERT",
      "shortName": "BERT",
      "company": "Google",
      "releaseDate": "2018-10-11",
      "parameters": "110M/340M",
      "parametersRaw": 340000000,
      "architecture": "BertModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["bert", "google", "encoder", "landmark"],
      "huggingfaceUrl": "https://huggingface.co/google-bert/bert-base-uncased",
      "paperUrl": "https://arxiv.org/abs/1810.04805",
      "description": "Bidirectional Encoder Representations from Transformers. Revolutionized NLP with masked language modeling.",
      "significance": "Dominated NLP benchmarks; sparked the pre-train → fine-tune paradigm",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "meta/roberta",
      "name": "RoBERTa",
      "shortName": "RoBERTa",
      "company": "Meta",
      "releaseDate": "2019-07-26",
      "parameters": "125M/355M",
      "parametersRaw": 355000000,
      "architecture": "RobertaModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "mit",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["roberta", "meta", "encoder"],
      "paperUrl": "https://arxiv.org/abs/1907.11692",
      "description": "Robustly Optimized BERT. Showed BERT was significantly undertrained.",
      "significance": "Proved that training recipe matters as much as architecture",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "huggingface/distilbert",
      "name": "DistilBERT",
      "shortName": "DistilBERT",
      "company": "Hugging Face",
      "releaseDate": "2019-10-02",
      "parameters": "66M",
      "parametersRaw": 66000000,
      "architecture": "DistilBertModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["distilbert", "huggingface", "distillation"],
      "paperUrl": "https://arxiv.org/abs/1910.01108",
      "description": "Distilled version of BERT that is 60% faster while retaining 97% of language understanding.",
      "significance": "Showed knowledge distillation works brilliantly for transformers",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/albert",
      "name": "ALBERT",
      "shortName": "ALBERT",
      "company": "Google",
      "releaseDate": "2019-09-26",
      "parameters": "12M/235M",
      "parametersRaw": 235000000,
      "architecture": "AlbertModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["albert", "google", "parameter-efficient"],
      "paperUrl": "https://arxiv.org/abs/1909.11942",
      "description": "A Lite BERT with parameter reduction techniques like factorized embeddings and cross-layer parameter sharing.",
      "significance": "Pioneered parameter-efficient transformer architectures",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/electra",
      "name": "ELECTRA",
      "shortName": "ELECTRA",
      "company": "Google",
      "releaseDate": "2020-03-23",
      "parameters": "14M/335M",
      "parametersRaw": 335000000,
      "architecture": "ElectraModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["electra", "google"],
      "paperUrl": "https://arxiv.org/abs/2003.10555",
      "description": "Replaced Masked LM with a replaced-token-detection pre-training task. More sample efficient than BERT.",
      "significance": "Showed alternative pre-training objectives can outperform MLM",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "microsoft/deberta",
      "name": "DeBERTa",
      "shortName": "DeBERTa",
      "company": "Microsoft",
      "releaseDate": "2020-06-05",
      "parameters": "134M/400M",
      "parametersRaw": 400000000,
      "architecture": "DebertaModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "mit",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["deberta", "microsoft", "disentangled-attention"],
      "paperUrl": "https://arxiv.org/abs/2006.03654",
      "description": "Decoding-enhanced BERT with Disentangled Attention. First to surpass human baseline on SuperGLUE.",
      "significance": "First model to beat human performance on SuperGLUE benchmark",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "microsoft/deberta-v3",
      "name": "DeBERTa v3",
      "shortName": "DeBERTa v3",
      "company": "Microsoft",
      "releaseDate": "2021-11-18",
      "parameters": "86M/304M",
      "parametersRaw": 304000000,
      "architecture": "DebertaV2Model",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "mit",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["deberta", "microsoft"],
      "paperUrl": "https://arxiv.org/abs/2111.09543",
      "description": "Improved DeBERTa with ELECTRA-style pre-training and gradient-disentangled embeddings.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "answerai/modernbert",
      "name": "ModernBERT",
      "shortName": "ModernBERT",
      "company": "Answer.AI",
      "releaseDate": "2024-12-19",
      "parameters": "149M/395M",
      "parametersRaw": 395000000,
      "architecture": "ModernBertModel",
      "architectureFamily": "Transformer (Encoder-only)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 8192,
      "tags": ["modernbert", "answerai", "encoder"],
      "description": "Modernized BERT with rotary embeddings, flash attention, and 8192 context trained on 2T tokens.",
      "significance": "Brought modern architectural improvements to the encoder paradigm",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/t5",
      "name": "T5",
      "shortName": "T5",
      "company": "Google",
      "releaseDate": "2019-10-23",
      "parameters": "60M-11B",
      "parametersRaw": 11000000000,
      "architecture": "T5ForConditionalGeneration",
      "architectureFamily": "Transformer (Encoder-Decoder)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["t5", "google", "encoder-decoder"],
      "paperUrl": "https://arxiv.org/abs/1910.10683",
      "description": "Text-to-Text Transfer Transformer. Unified all NLP tasks into text-to-text format.",
      "significance": "Unified NLP under a single text-to-text paradigm",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/mt5",
      "name": "mT5",
      "shortName": "mT5",
      "company": "Google",
      "releaseDate": "2020-10-22",
      "parameters": "300M-13B",
      "parametersRaw": 13000000000,
      "architecture": "T5ForConditionalGeneration",
      "architectureFamily": "Transformer (Encoder-Decoder)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["t5", "google", "multilingual"],
      "paperUrl": "https://arxiv.org/abs/2010.11934",
      "description": "Multilingual T5 pre-trained on mC4 covering 101 languages.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/flan-t5",
      "name": "Flan-T5",
      "shortName": "Flan-T5",
      "company": "Google",
      "releaseDate": "2022-10-20",
      "parameters": "80M-11B",
      "parametersRaw": 11000000000,
      "architecture": "T5ForConditionalGeneration",
      "architectureFamily": "Transformer (Encoder-Decoder)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["t5", "google", "instruction-tuned", "flan"],
      "paperUrl": "https://arxiv.org/abs/2210.11416",
      "description": "T5 fine-tuned on 1,836 tasks phrased as instructions. Massively improved zero-shot performance.",
      "significance": "Demonstrated the power of instruction tuning at scale",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/ul2",
      "name": "UL2",
      "shortName": "UL2",
      "company": "Google",
      "releaseDate": "2022-05-10",
      "parameters": "20B",
      "parametersRaw": 20000000000,
      "architecture": "T5ForConditionalGeneration",
      "architectureFamily": "Transformer (Encoder-Decoder)",
      "modality": "text",
      "license": "apache-2.0",
      "licenseType": "open-source",
      "contextWindow": 512,
      "tags": ["ul2", "google", "unified"],
      "paperUrl": "https://arxiv.org/abs/2205.05131",
      "description": "Unifying Language Learning Paradigms. Combined various pre-training objectives.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/palm",
      "name": "PaLM",
      "shortName": "PaLM",
      "company": "Google",
      "releaseDate": "2022-04-04",
      "parameters": "540B",
      "parametersRaw": 540000000000,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 2048,
      "tags": ["palm", "google", "landmark"],
      "paperUrl": "https://arxiv.org/abs/2204.02311",
      "description": "540B parameter model demonstrating breakthrough chain-of-thought reasoning.",
      "significance": "Showed emergent chain-of-thought reasoning in very large models",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/palm-2",
      "name": "PaLM 2",
      "shortName": "PaLM 2",
      "company": "Google",
      "releaseDate": "2023-05-10",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 8192,
      "tags": ["palm", "google"],
      "paperUrl": "https://arxiv.org/abs/2305.10403",
      "description": "Improved PaLM with compute-optimal scaling and multilingual training.",
      "significance": "Powered Bard/Gemini and Google's AI products",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/med-palm-2",
      "name": "Med-PaLM 2",
      "shortName": "Med-PaLM 2",
      "company": "Google",
      "releaseDate": "2023-05-16",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only)",
      "modality": "text",
      "license": "proprietary",
      "licenseType": "closed-source",
      "tags": ["palm", "google", "medical"],
      "paperUrl": "https://arxiv.org/abs/2305.09617",
      "description": "Medical domain fine-tune of PaLM 2. Achieved expert-level medical knowledge.",
      "significance": "First AI to reach expert-level medical exam performance",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-1.0",
      "name": "Gemini 1.0",
      "shortName": "Gemini 1.0",
      "company": "Google",
      "releaseDate": "2023-12-06",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, Multimodal)",
      "modality": "text+vision+audio",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 32768,
      "tags": ["gemini", "google", "multimodal", "landmark"],
      "blogUrl": "https://blog.google/technology/ai/google-gemini-ai/",
      "description": "Google's natively multimodal model family built from the ground up.",
      "significance": "Google's answer to GPT-4; first natively multimodal from the ground up",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-1.0-pro",
      "name": "Gemini 1.0 Pro",
      "shortName": "Gemini Pro",
      "company": "Google",
      "releaseDate": "2023-12-13",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, Multimodal)",
      "modality": "text+vision",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 32768,
      "tags": ["gemini", "google"],
      "description": "Mid-tier Gemini model balancing performance and efficiency.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-1.0-ultra",
      "name": "Gemini 1.0 Ultra",
      "shortName": "Gemini Ultra",
      "company": "Google",
      "releaseDate": "2024-02-08",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, Multimodal)",
      "modality": "text+vision+audio",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 32768,
      "tags": ["gemini", "google", "flagship"],
      "description": "The largest and most capable Gemini 1.0 model.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-1.5-pro",
      "name": "Gemini 1.5 Pro",
      "shortName": "Gemini 1.5 Pro",
      "company": "Google",
      "releaseDate": "2024-02-15",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, MoE, Multimodal)",
      "modality": "text+vision+audio+video",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 2000000,
      "tags": ["gemini", "google", "long-context"],
      "paperUrl": "https://arxiv.org/abs/2403.05530",
      "description": "Gemini 1.5 Pro with 1M+ token context window using MoE architecture.",
      "significance": "Pioneered million-token context windows",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-1.5-flash",
      "name": "Gemini 1.5 Flash",
      "shortName": "Gemini 1.5 Flash",
      "company": "Google",
      "releaseDate": "2024-05-14",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, MoE, Multimodal)",
      "modality": "text+vision+audio+video",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 1000000,
      "tags": ["gemini", "google", "fast"],
      "description": "Lightweight, fast Gemini optimized for speed and efficiency.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-2.0-flash",
      "name": "Gemini 2.0 Flash",
      "shortName": "Gemini 2.0 Flash",
      "company": "Google",
      "releaseDate": "2024-12-11",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, MoE, Multimodal)",
      "modality": "text+vision+audio+video",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 1000000,
      "tags": ["gemini", "google", "agentic"],
      "description": "Next-gen Flash with agentic capabilities and multimodal output.",
      "significance": "First Gemini with native tool use and agentic features",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-2.5-pro",
      "name": "Gemini 2.5 Pro",
      "shortName": "Gemini 2.5 Pro",
      "company": "Google",
      "releaseDate": "2025-03-25",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, MoE, Multimodal)",
      "modality": "text+vision+audio+video",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 1000000,
      "tags": ["gemini", "google", "thinking"],
      "description": "Google's thinking model with built-in reasoning capabilities.",
      "significance": "Google's answer to reasoning models (o1/o3)",
      "source": "curated",
      "lastVerified": "2025-06-15"
    },
    {
      "id": "google/gemini-2.5-flash",
      "name": "Gemini 2.5 Flash",
      "shortName": "Gemini 2.5 Flash",
      "company": "Google",
      "releaseDate": "2025-04-17",
      "parameters": "Unknown",
      "parametersRaw": null,
      "architecture": "TransformerDecoder",
      "architectureFamily": "Transformer (Decoder-only, MoE, Multimodal)",
      "modality": "text+vision+audio+video",
      "license": "proprietary",
      "licenseType": "closed-source",
      "contextWindow": 1000000,
      "tags": ["gemini", "google", "thinking", "fast"],
      "description": "Fast thinking model combining speed with reasoning.",
      "source": "curated",
      "lastVerified": "2025-06-15"
    }
  ],
  "relationships": [
    { "parent": "google/transformer", "child": "google/bert", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/transformer", "child": "openai/gpt-1", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/transformer", "child": "google/t5", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "meta/roberta", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "huggingface/distilbert", "type": "distillation", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "google/albert", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "google/electra", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "microsoft/deberta", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "microsoft/deberta", "child": "microsoft/deberta-v3", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/bert", "child": "answerai/modernbert", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/t5", "child": "google/mt5", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/t5", "child": "google/flan-t5", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "google/t5", "child": "google/ul2", "type": "architecture", "source": "curated", "confidence": "high" },
    { "parent": "google/palm", "child": "google/palm-2", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/palm-2", "child": "google/med-palm-2", "type": "finetune", "source": "curated", "confidence": "high" },
    { "parent": "google/palm", "child": "google/gemini-1.0", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-1.0", "child": "google/gemini-1.0-pro", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-1.0", "child": "google/gemini-1.0-ultra", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-1.0", "child": "google/gemini-1.5-pro", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-1.5-pro", "child": "google/gemini-1.5-flash", "type": "distillation", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-1.5-pro", "child": "google/gemini-2.0-flash", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-2.0-flash", "child": "google/gemini-2.5-pro", "type": "base", "source": "curated", "confidence": "high" },
    { "parent": "google/gemini-2.5-pro", "child": "google/gemini-2.5-flash", "type": "distillation", "source": "curated", "confidence": "high" }
  ]
}
